'use strict';

const _ = require("lodash");
const validatorUtil = require("../../utils/validatorUtil");
const helper = require("../../utils/helper");
const errorHelper = require("../../utils/errors");
const fs = require("fs");
const config = require("../../config/data_models_storage_config.json")["<%- database -%>"];
const amazonS3Helper = require("../../utils/amazonS3_helper");
const path = require("path");
const uuidv4 = require("uuidv4").uuid;
const os = require("os");

// An exact copy of the the model definition that comes from the .json file
const definition = <%- definition -%>;
const DataLoader = require("dataloader");
const params = {
    Bucket: config.bucket,
    Key: "<%- nameLc -%>.csv",
    ExpressionType: "SQL",
    InputSerialization: {
      CSV: {
        FileHeaderInfo: config.fileHeaderInfo,
        RecordDelimiter: config.recordDelimiter,
        FieldDelimiter: config.fieldDelimiter,
      },
      CompressionType: "NONE",
    },
    OutputSerialization: {
      JSON: {
        RecordDelimiter: ",",
      },
    },
};
/**
 * module - Creates a class to administer model
 */
module.exports = class <%- nameLc -%> {
  constructor(input) {
    for (let key of Object.keys(input)) {
      this[key] = input[key];
    }
  }

  get storageHandler() {
    return <%- nameLc -%>.storageHandler
  }
 
  /**
  * name - Getter for the name attribute
  *
  * This attribute is needed by the models' index
  * @return {string} The name of the model
  */
  static get name(){
    return "<%- nameLc -%>";
  }

  /**
   * Split string to array for the validation.
   * @param  {object} record  Input Record.
   * @return {object}         Parsed data record.
   */
  static postReadCast(record) {
    for (let attr in definition.attributes) {
      let type = definition.attributes[attr].replace(/\s+/g, "");
      if (type[0] === "[") {
        record[attr] = record[attr]
          .split(config.arrayDelimiter)
          .map((field) =>
          this.typeConversion(field, type.slice(1, type.length - 1))
          );
        if (record[attr].length===1 && record[attr][0]===null){
          record[attr] = null;
        }
      } else {
        record[attr] = this.typeConversion(record[attr], type);
      }
    }
    return record;
  }

  /**
   * Cast field to corresponding data type.
   * @param  {string} field  Value in the field.
   * @param  {string} type   Datatype for current field.
   * @return {null|int|bool|float|string} Parsed data field.
   */
  static typeConversion(field, type) {
    let stringType = ["String", "Date", "Time", "DateTime"];
    if (stringType.includes(type)) {
      return field.replace(/\s+/g, "") === "" || field.toLowerCase() === "null"
      ? null
      : field;
    } else if (type === "Int") {
      return parseInt(field);
    } else if (type === "Float") {
      return parseFloat(field);
    } else if (type === "Boolean") {
      return field === "true";
    } else {
      console.log("No support for current type. Please check your input.");
      return null;
    }
  }

  <% if (useDataLoader) {%>/**
    * Batch function for readById method.
    * @param  {array} keys  keys from readById method
    * @return {array}       searched results
    */
   static async batchReadById(keys) {
     let queryArg = {
       operator: "in",
       field: <%- nameLc -%>.idAttribute(),
       value: keys.join(),
       valueType: "Array",
     };
     let cursorRes = await <%- nameLc -%>.readAllCursor(queryArg);
     cursorRes = cursorRes.<%- namePl -%>.reduce(
       (map, obj) => ((map[obj[<%- nameLc -%>.idAttribute()]] = obj), map),
       {}
     );
     return keys.map(
       (key) =>
         cursorRes[key] || new Error(`Record with ID = "${key}" does not exist`)
     );
   }
 
   static readByIdLoader = new DataLoader(<%- nameLc -%>.batchReadById, {
     cache: false,
   });

   /**
   * readById - The model implementation for reading a single record given by its ID
   *
   * This method is the implementation for reading a single record for the Amazon S3 storage type, based on SQL.
   * @param {string} id - The ID of the requested record
   * @return {object} The requested record as an object with the type <%- nameLc -%>, or an error object if the validation after reading fails
   * @throws {Error} If the requested record does not exist
   */
   static async readById(id) {
     return await <%- nameLc -%>.readByIdLoader.load(id);
   }<% } else 
  {%>/**
  * readById - The model implementation for reading a single record given by its ID
  *
  * This method is the implementation for reading a single record for the Amazon S3 storage type, based on SQL.
  * @param {string} id - The ID of the requested record
  * @return {object} The requested record as an object with the type <%- nameLc -%>, or an error object if the validation after reading fails
  * @throws {Error} If the requested record does not exist
  */
  static async readById(id){
    const query = {
      ...params,
      Expression: `SELECT * FROM S3Object WHERE ${this.idAttribute()} = '${id}'`,
    };
    let item = null;
    try {
      const s3 = await this.storageHandler;
      const result = await s3.selectObjectContent(query);
      const events = result.Payload;
      for await (const event of events) {
        // Check the top-level field to determine which event this is.
        if (event.Records) {
         // handle Records event
          item = Buffer.from(event.Records.Payload).toString();
          item = JSON.parse(item.slice(0, item.length - 1));
        }
    }
    if (!item) {
      throw new Error(`Record with ID = "${id}" does not exist`);
    }
    } catch (e) {
      throw new Error(e);
    }
    item = new <%- nameLc -%>(<%- nameLc -%>.postReadCast(item));
    return validatorUtil.validateData("validateAfterRead", this, item);
  }<%}-%>

  /**
  * countRecords - The model implementation for counting the number of records, possibly restricted by a search term
  *
  * This method is the implementation for counting the number of records that fulfill a given condition, or for all records in the table,
  * for the Amazon S3 storage type, based on SQL.
  * @param {object} search - The search term that restricts the set of records to be counted - if undefined, all records in the table
  * @param {BenignErrorReporter} benignErrorReporter can be used to generate the standard
  * @return {number} The number of records that fulfill the condition, or of all records in the table
  */
  static async countRecords(search, benignErrorReporter){
    const whereOptions = amazonS3Helper.searchConditionsToAmazonS3(
      search,
      definition,
      config.arrayDelimiter
    );
    const query = {
      ...params,
      Expression: `SELECT COUNT(*) AS num FROM S3Object ${whereOptions}`,
    };
    let num = null;
    try {
      const s3 = await this.storageHandler;
      const result = await s3.selectObjectContent(query);
      const events = result.Payload;
      for await (const event of events) {
        // Check the top-level field to determine which event this is.
        if (event.Records) {
          // handle Records event
          num = Buffer.from(event.Records.Payload).toString();
          num = JSON.parse(num.slice(0, num.length - 1)).num;
        }
      }
    } catch (e) {
      throw new Error(e);
    }
    return num;
  }

  /**
  * readAll - Limit-offset based pagination is not offered by Amazon S3, and this method is left here only as information
  * to the user / developer. Use *readAllCursor* instead, which relies on cursor based pagination.
  * @throw {Error} If this method is used at all, an Error is thrown
  */
  static readAll(search, order, pagination, benignErrorReporter){
    throw new Error('Limit-offset based pagination is not supported by Amazon S3');
  }

  /**
  * readAllCursor - The model implementation for searching for records in Amazon S3. This method uses cursor based pagination.
  *
  * @param {object} search - The search condition for which records shall be fetched
  * @param {object} pagination - The parameters for pagination, which can be used to get a subset of the requested record set.
  * @param {BenignErrorReporter} benignErrorReporter can be used to generate the standard
  * @return {object} The set of records, possibly constrained by pagination, with full cursor information for all records
  */
  static async readAllCursor(search, order, pagination, benignErrorReporter){
    if (order) {
      console.log(`order would be ignored because S3 doesn't support sorting`);
    }
    let amazonS3Search =
     pagination && pagination.after
      ? amazonS3Helper.cursorPaginationArgumentsToAmazonS3(
        search,
        pagination,
        this.idAttribute()
        )
        : search;
    let whereOptions = amazonS3Helper.searchConditionsToAmazonS3(
      amazonS3Search,
      definition,
      config.arrayDelimiter
    );
    let query = `SELECT * FROM S3Object ${whereOptions} `;
  
    if (pagination && pagination.first) {
      query += ` LIMIT ${pagination.first+1}`;
    }
  
    let records = [];
    try {
      const s3 = await this.storageHandler;
      const result = await s3
        .selectObjectContent({
          ...params,
          Expression: query,
          });
      const events = result.Payload;
      for await (const event of events) {
        // Check the top-level field to determine which event this is.
        if (event.Records) {
          // handle Records event
          records = Buffer.from(event.Records.Payload).toString();
          records = JSON.parse("[" + records.slice(0, records.length - 1) + "]");
        }
      }
    } catch (e) {
      throw new Error(e);
    }
  
    // Construct return object
    records = records.map( row => { return new <%- nameLc -%>(<%- nameLc -%>.postReadCast(row))})
    let rows = records.map( row => {
        return {
          node: row,
          cursor: row.base64Encode(),
        }
    })
  
    let startCursor = null;
    let nextCursor = null;
    let hasNextCursor = false;
  
    if (pagination && pagination.first && rows.length > pagination.first) {
      rows.pop();
      startCursor = rows[0].cursor;
      nextCursor = rows[rows.length - 1].cursor;
      hasNextCursor = true;
    }
  
    let pageInfo = {
      startCursor: startCursor,
      endCursor: nextCursor,
      hasNextPage: hasNextCursor,
      hasPreviousPage: false,
    };
    return { edges: rows, pageInfo: pageInfo, <%- namePl -%>: rows.map((edge) => edge.node)};
  }

  /**
  * addOne - Not implemented for AmazonS3.
  */
  static async addOne(input){
    throw new Error('Not supported by Amazon S3');
  }

  /**
  * deleteOne - Delete the whole file.
  */
  static async deleteOne(id){
    throw new Error('Not supported by Amazon S3');
  }

  /**
  * updateOne - Not implemented for AmazonS3.
  */
  static async updateOne(input){
    throw new Error('Not supported by Amazon S3');
  }

  /**
   * csvTableTemplate - Allows the user to download a template in CSV format with the
   * properties and types of this model.
   *
   * @param {BenignErrorReporter} benignErrorReporter can be used to generate the standard
   * GraphQL output {error: ..., data: ...}. If the function reportError of the benignErrorReporter
   * is invoked, the server will include any so reported errors in the final response, i.e. the
   * GraphQL response will have a non empty errors property.
   */
  static async csvTableTemplate(benignErrorReporter){
      return helper.csvTableTemplate(definition);
  }
  
  <%- include('./includes/create-models-functions', {model: nameLc}); %>

}